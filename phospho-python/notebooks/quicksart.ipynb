{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phospho quickstart\n",
    "\n",
    "In this quickstart, we will use the `lab` from the `phospho` package to figure out how many messages in a dataset are questions. \n",
    "\n",
    "\n",
    "1. First, we will detect events on a subset of the dataset using a pipeline powered by OpenAI GPT 3.5\n",
    "\n",
    "2. Then, we will scale analytics with the `lab` optimizer. We will compare the event detection pipeline using MistralAI and a local Ollama model, and pick the best one in term of performance, speed and price.\n",
    "\n",
    "3. Finally, we will use the `lab` to run the best model on the full dataset and visualize the results. \n",
    "\n",
    "This way, we will be able to run semantic analytics at scale on a dataset using LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.9.6.post2 requires urllib3<2, but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q python-dotenv \"phospho[lab]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check env variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from phospho import config\n",
    "\n",
    "assert config.OPENAI_API_KEY is not None, \"You need to set the OPENAI_API_KEY environment variable\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Event detection pipeline\n",
    "\n",
    "In phospho, there are two important concepts:\n",
    "- A workload, which is a set of jobs. Those jobs are run asynchronously and in parallel.\n",
    "- A job, which is a python function that returns a JobResult. Jobs are parametrized with a JobConfig.\n",
    "\n",
    "In this example, the Job is to detect an event (\"Event Detection\") using LLM self-reflection (we asked another LLM whether the event occured or not). We will try to detect the event: \"The user asks a question to the assistant\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import lab\n",
    "\n",
    "# Create a workload in our lab\n",
    "workload = lab.Workload()\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"sync_event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=lab.EventConfig(\n",
    "            event_name=\"Question Answering\",\n",
    "            event_description=\"User asks a question to the assistant\",\n",
    "            model_id=\"openai:gpt-3.5-turbo\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can run the pipeline on a Message. \n",
    "\n",
    "We want to detect whether the user asks a question. Sometimes, it's easy: there is a question mark. But sometimes, it's not: you understand that it is a question only through context and semantics. That's why you need an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In message 1, the Event question answering was detected: True\n",
      "In message 2, the Event question answering was detected: False\n",
      "In message 3, the Event question answering was detected: True\n"
     ]
    }
   ],
   "source": [
    "await workload.async_run(\n",
    "    messages=[\n",
    "        # This message is a question, very simple to detect. \n",
    "        lab.Message(\n",
    "            id=\"message_1\",\n",
    "            content=\"What is the capital of France?\",\n",
    "        ),\n",
    "        # This message is not a question, so it should not be detected.\n",
    "        lab.Message(\n",
    "            id=\"message_2\",\n",
    "            content=\"I don't like croissants.\",\n",
    "        ),\n",
    "        # This message is also a question, but it lacks a question mark. You need semantics to detect it.\n",
    "        lab.Message(\n",
    "            id=\"message_3\",\n",
    "            content=\"I wonder what's the capital of France...\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    print(f\"In message {i}, the Event question answering was detected: {workload.results['message_'+str(i)]['question_answering'].value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analytics\n",
    "\n",
    "Now, let's assume we want to find user questions in a large dataset. How would we do it?\n",
    "\n",
    "Let's load a dataset of messages from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daily_dialog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has more than 10 000 samples. That's a lot and running analytics on it can quickly become pricy. So let's just select a subsample of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialog', 'act', 'emotion'],\n",
       "    num_rows: 11118\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say , Jim , how about going for a few beers after dinner ? \n"
     ]
    }
   ],
   "source": [
    "# Generate a sub dataset with 30 messages\n",
    "sub_dataset = dataset[\"train\"].select(range(30))\n",
    "\n",
    "# Let's print one of the messages\n",
    "print(sub_dataset[0][\"dialog\"][0])\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the analytics pipeline on the subset. \n",
    "\n",
    "The workload run is asynchronous and parallelized, which means this will go much faster than just writing a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the lab on it\n",
    "# The job will be runned with the default model (openai:gpt-3.5-turbo)\n",
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 583a6fe9be6d44c0a8ec08bfb5a9d2c3 was a question: False\n",
      "Message fad1117dd5874e84a8684737f727bb4c was a question: True\n",
      "Message ed893f49ff7248669e800efb58609239 was a question: True\n",
      "Message b972a51a7a9c4dd992f259e2f63b3333 was a question: False\n",
      "Message 95efb28a32a14257809aad73d31e4899 was a question: False\n",
      "Message e4e7a57c06c146c49d1ec1d53add836b was a question: False\n",
      "Message 4690a83bf95a44a4b676fc1cbf1c55b9 was a question: False\n",
      "Message 49b839ebd5cc4652809e535f4a482754 was a question: False\n",
      "Message f9ee6e9df9d1440fadbb076812a840f5 was a question: False\n",
      "Message 4918792b6d7f4bd18112ccd46f9f2c75 was a question: True\n",
      "Message 71eaf5f02823413cb5624b7cc39833a8 was a question: True\n",
      "Message 7cb2cc2410064649978bbfb319636131 was a question: False\n",
      "Message 8b1d0af4eab24f6ba121e0b85fa4591e was a question: False\n",
      "Message 38ba128d9ef440e7ac4769dcd4afa182 was a question: True\n",
      "Message dcc6019547104f648f3ef4aa88ab7b0f was a question: False\n",
      "Message c71bcd2941b4499d94c17a4a539c03cc was a question: False\n",
      "Message f3c1a94a07a249dba0d63525827a444f was a question: True\n",
      "Message b075297a02504bb58ea4067a21d9cdde was a question: True\n",
      "Message 40ea77bdfc354e16b5b8690a34e57102 was a question: True\n",
      "Message c611d930c7fa481b836134d97ca3156b was a question: True\n",
      "Message c79368de92aa481f8f1abe4e4747838d was a question: False\n",
      "Message 7ad3ed3244944b95a206cdd6185ae41a was a question: True\n",
      "Message 723b963ff8294b7d861477aede67c1d0 was a question: True\n",
      "Message 135e755d3a33469594b764a72f872238 was a question: False\n",
      "Message 5e86e8efafc34b9a9dab8ed8de15c426 was a question: True\n",
      "Message 7a380b95ec12412c8b9ba75d4e1b449b was a question: False\n",
      "Message 37e6a8a668384f45b8636c79ec86c429 was a question: True\n",
      "Message f2246341978044ec94c6ea1426f4d355 was a question: True\n",
      "Message df5960320bbe4384811a622c146d1f05 was a question: False\n",
      "Message 82a9585320214946a34136f376b4097b was a question: True\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for message_id, jobs in workload_results.items():\n",
    "    print(f\"Message {message_id} was a question: {jobs['question_answering'].value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the pipeline\n",
    "\n",
    "Running semantic analytics with an LLM is great. But it's expensive and slow.\n",
    "\n",
    "You likely want to try other model providers, such as Mistral, or even small local models. But how do they compare?\n",
    "\n",
    "Let's run the pipeline on these models, and then figure out which one matches the reference, GPT-4. \n",
    "\n",
    "For the purpose of this demo, we consider a considertion good enough if it matches gpt-4 on at least 80% of the dataset. Good old Paretto.\n",
    "\n",
    "### Installation and setup\n",
    "\n",
    "You will need:\n",
    "- a Mistral AI API key (find yours [here](https://console.mistral.ai/api-keys/))\n",
    "- Ollama running on your local machine, with the Mistral 7B model installed. You can find the installation instructions for Ollama [here](https://ollama.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import config\n",
    "\n",
    "# Check the environment variable\n",
    "assert config.MISTRAL_API_KEY is not None, \"You need to set the MISTRAL_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Camembert\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "try:\n",
    "  # Let's check we can reach your local Ollama API\n",
    "  response = ollama.chat(model='mistral', messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is the best French cheese? Keep your answer short.',\n",
    "    },\n",
    "  ])\n",
    "  print(response['message']['content'])\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")\n",
    "  print(\"You need to have a local Ollama server running to continue and the mistral model downloaded. \\nRemove references to Ollama otherwise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the results with the alternative configurations\n",
    "\n",
    "To run the jobs on multiple models at the same time, we will simply set up our job with a different configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "workload = lab.Workload()\n",
    "\n",
    "# Setup the configs for our job\n",
    "class EventConfig(lab.JobConfig):\n",
    "    event_name: str\n",
    "    event_description: str\n",
    "    # Model are ordered from the least desired to the most desired\n",
    "    # The default model is set to be the \"reference\"\n",
    "    model_id: Literal[\"openai:gpt-4\", \"mistral:mistral-large-latest\", \"mistral:mistral-small-latest\", \"ollama:mistral-7B\"] = \"openai:gpt-4\"\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"sync_event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=EventConfig(\n",
    "            event_name=\"Question Answering\",\n",
    "            event_description=\"User asks a question to the assistant\",\n",
    "            model_id=\"openai:gpt-4\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the workload in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai:gpt-4'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the default model is currently set to \"openai:gpt-4\"\n",
    "workload.jobs[0].config.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's also run the pipeline on alternative models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alternative results with the Mistral API and Ollama\n",
    "await workload.async_run_on_alternative_configurations(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the workload to figure out which model is better. \n",
    "\n",
    "Note that this can actually work with any set of parameters, not just models. It's a flexible way to perform a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:24:35,159 INFO phospho.lab.lab: Found a less costly config with accuracy of 0.8. Swapping to it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: [0.8666666666666667, 0.8, 0.6]\n"
     ]
    }
   ],
   "source": [
    "workload.optimize_jobs(accuracy_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what model was picked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral:mistral-small-latest'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the new model_id (if it has changed)\n",
    "workload.jobs[0].config.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can run our pipeline with roughly the same accuracy using a much smaller model. That's a lot of time, compute and money saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our workload on the full dataset, with optimized parameters\n",
    "\n",
    "Now that we have benchmarked different models for our Event detection pipeline, let's run the optimal configuration on a larger chunk of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset[\"train\"].select(range(200)) # Here you can just leave it as dataset[\"train\"] if you want to use the whole dataset\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The job will be runned with the best model (mistral:mistral-small-latest in our case)\n",
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results\n",
    "\n",
    "Now, we were trying to see which share of the dataset is actually a question. Let's get the results as a dataframe and visualize them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_answering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fa4c80d52bcf46d2a39575c952d7c150</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5d6f30cf533452f8e931aa5fe9ff1b3</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2b34956edcb4c0d91315176835ea174</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160686f19df843939764f22a4c54c0cf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6811f4a779d046e8b88b017d44ff5ead</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2d5db195448644bcb02aa0ef78a3fe12</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603020bb7214467b93d95eb1af81e29</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fcc314ed2941444da6639bb41f4ea9b0</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>053f18b30be04f00a6b61f161e4410b7</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92e884555f624cdeb7716a8f1b6be65c</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question_answering\n",
       "fa4c80d52bcf46d2a39575c952d7c150               False\n",
       "f5d6f30cf533452f8e931aa5fe9ff1b3               False\n",
       "c2b34956edcb4c0d91315176835ea174                True\n",
       "160686f19df843939764f22a4c54c0cf               False\n",
       "6811f4a779d046e8b88b017d44ff5ead               False\n",
       "...                                              ...\n",
       "2d5db195448644bcb02aa0ef78a3fe12                True\n",
       "2603020bb7214467b93d95eb1af81e29                True\n",
       "fcc314ed2941444da6639bb41f4ea9b0                True\n",
       "053f18b30be04f00a6b61f161e4410b7                True\n",
       "92e884555f624cdeb7716a8f1b6be65c               False\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using .results_df() we can get a pandas dataframe with the results\n",
    "workload.results_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many questions in this dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_answering\n",
       "False    111\n",
       "True      89\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='question_answering'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "You can use the `lab` to run other tasks, such as:\n",
    "- Named Entity Recognition\n",
    "- Sentiment Analysis\n",
    "- Evaluations\n",
    "- And more!\n",
    "\n",
    "You can also play around with differnet models, different hyperparameters, and different datasets.\n",
    "\n",
    "You want to have such analysis on your own LLM app, in real time? Check out the cloud hosted version of phospho, available on [phospho.ai](https://phospho.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
