{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phospho quickstart\n",
    "\n",
    "In this quickstart, we will use the `lab` from the `phospho` package to figure out how many messages in a dataset are questions. \n",
    "\n",
    "1. First, we will detect events on a subset of the dataset using a pipeline powered by OpenAI GPT 3.5\n",
    "\n",
    "2. Then, we will scale analytics with the `lab` optimizer. We will compare the event detection pipeline using MistralAI and a local Ollama model, and pick the best one in term of performance, speed and price.\n",
    "\n",
    "3. Finally, we will use the `lab` to run the best model on the full dataset and visualize the results. \n",
    "\n",
    "This way, we will be able to run semantic analytics at scale on a dataset using LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q python-dotenv \"phospho[lab]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check env variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from phospho import config\n",
    "\n",
    "assert config.OPENAI_API_KEY is not None, \"You need to set the OPENAI_API_KEY environment variable\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Event detection pipeline\n",
    "\n",
    "In phospho, there are two important concepts:\n",
    "- A workload, which is a set of jobs. Those jobs are run asynchronously and in parallel.\n",
    "- A job, which is a python function that returns a JobResult. Jobs are parametrized with a JobConfig.\n",
    "\n",
    "In this example, the Job is to detect an event (\"Event Detection\") using LLM self-reflection (we asked another LLM whether the event occured or not). We will try to detect the event: \"The user asks a question to the assistant\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import lab\n",
    "\n",
    "# Create a workload in our lab\n",
    "workload = lab.Workload()\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=lab.EventConfig(\n",
    "            event_name=\"Question Answering\",\n",
    "            event_description=\"User asks a question to the assistant\",\n",
    "            model=\"openai:gpt-3.5-turbo\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can run the pipeline on a Message. \n",
    "\n",
    "We want to detect whether the user asks a question. Sometimes, it's easy: there is a question mark. But sometimes, it's not: you understand that it is a question only through context and semantics. That's why you need an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In message 1, the Event question answering was detected: True\n",
      "In message 2, the Event question answering was detected: False\n",
      "In message 3, the Event question answering was detected: True\n"
     ]
    }
   ],
   "source": [
    "await workload.async_run(\n",
    "    messages=[\n",
    "        # This message is a question, very simple to detect. \n",
    "        lab.Message(\n",
    "            id=\"message_1\",\n",
    "            content=\"What is the capital of France?\",\n",
    "        ),\n",
    "        # This message is not a question, so it should not be detected.\n",
    "        lab.Message(\n",
    "            id=\"message_2\",\n",
    "            content=\"I don't like croissants.\",\n",
    "        ),\n",
    "        # This message is also a question, but it lacks a question mark. You need semantics to detect it.\n",
    "        lab.Message(\n",
    "            id=\"message_3\",\n",
    "            content=\"I wonder what's the capital of France...\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    print(f\"In message {i}, the Event question answering was detected: {workload.results['message_'+str(i)]['question_answering'].value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analytics\n",
    "\n",
    "Now, let's assume we want to find user questions in a large dataset. How would we do it?\n",
    "\n",
    "Let's load a dataset of messages from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.9.6.post2 requires tiktoken>=0.3.3, which is not installed.\n",
      "llama-index 0.9.6.post2 requires urllib3<2, but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daily_dialog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has more than 10 000 samples. That's a lot and running analytics on it can quickly become pricy. So let's just select a subsample of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialog', 'act', 'emotion'],\n",
       "    num_rows: 11118\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say , Jim , how about going for a few beers after dinner ? \n"
     ]
    }
   ],
   "source": [
    "# Generate a sub dataset with 30 messages\n",
    "sub_dataset = dataset[\"train\"].select(range(30))\n",
    "\n",
    "# Let's print one of the messages\n",
    "print(sub_dataset[0][\"dialog\"][0])\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the analytics pipeline on the subset. \n",
    "\n",
    "The workload run is **asynchronous** and **parallelized**, which means this will go much faster than just writing a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the lab on it\n",
    "# The job will be run with the default model (openai:gpt-3.5-turbo)\n",
    "workload_results = await workload.async_run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 77dc8427821a4dada22beb6c9a3becda was a question: False\n",
      "Message 105c1eb1d9f1424ea22ca85caaed8032 was a question: True\n",
      "Message e3bd8de153da4de6b1bdce94375f914b was a question: True\n",
      "Message c710e6f1ffcd44bc9de5d7436d08b4a7 was a question: False\n",
      "Message 448d63564748440c986eb7e73e5f312e was a question: False\n",
      "Message 95b8f62d82fb4a7d918e706bad488f04 was a question: True\n",
      "Message d7d819fb98e647ae912d74c769659d61 was a question: False\n",
      "Message f11e87af94c9456ea7bf6d8ae24e587a was a question: False\n",
      "Message 1f4a7597561b4667b38002a53baefe2f was a question: True\n",
      "Message ee7d0c911e44494da363ec1eb16f1e8b was a question: True\n",
      "Message 084acb47636f45348f0ec7588c8cf16f was a question: True\n",
      "Message 571902cf9ab84dcc8b609b02f2f092a0 was a question: False\n",
      "Message 673762a4a2d94f94904588b826286754 was a question: False\n",
      "Message 2bfcbc31b4ce459eb7014f4da63e0e38 was a question: True\n",
      "Message 70cae17e2c62473485a214fcfba91274 was a question: False\n",
      "Message 90757f1818bc45deb2a97aac263d6708 was a question: False\n",
      "Message bd115c474cac4a05a38faa966003f327 was a question: True\n",
      "Message 479e64a24d794e309b2441aeaa588915 was a question: True\n",
      "Message 8e48be283d0b4fd18fd27d640b57a098 was a question: True\n",
      "Message 2f419082de5d4837b68de9f09f71e26e was a question: True\n",
      "Message 8905edc9b29c458fbd8b18e3a6abb7a1 was a question: False\n",
      "Message 994cdf594dab443489d63d66dcaa8243 was a question: True\n",
      "Message 532e198067aa4e43bf5e27675a8c0732 was a question: True\n",
      "Message 831e457044c84bcaa81b03afffffbc5a was a question: False\n",
      "Message 11c186a06c914b2481896b7734f9d612 was a question: True\n",
      "Message d3c4290e40b044e2997d3756268476ed was a question: False\n",
      "Message 628d167b262a472ead581a2f52825c2a was a question: True\n",
      "Message 3ace7d7098a74cc8b5da950b28bb5c5a was a question: False\n",
      "Message fd82c1e9f5fb495a82d20ec4e69e4ae1 was a question: False\n",
      "Message 1a368ff42dd74c048b1322605920b1ab was a question: True\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for message_id, jobs in workload_results.items():\n",
    "    print(f\"Message {message_id} was a question: {jobs['question_answering'].value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the pipeline\n",
    "\n",
    "Running semantic analytics with an LLM is great. But it's expensive and slow.\n",
    "\n",
    "You likely want to try other model providers, such as Mistral, or even small local models. But how do they compare?\n",
    "\n",
    "Let's run the pipeline on these models, and then figure out which one matches the reference, GPT-4. \n",
    "\n",
    "For the purpose of this demo, we consider a considertion good enough if it matches gpt-4 on at least 80% of the dataset. Good old Paretto.\n",
    "\n",
    "### Installation and setup\n",
    "\n",
    "You will need:\n",
    "- a Mistral AI API key (find yours [here](https://console.mistral.ai/api-keys/))\n",
    "- Ollama running on your local machine, with the Mistral 7B model installed. You can find the installation instructions for Ollama [here](https://ollama.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import config\n",
    "\n",
    "# Check the environment variable\n",
    "assert config.MISTRAL_API_KEY is not None, \"You need to set the MISTRAL_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best French cheese varies by personal taste and region. But some renowned options include Roquefort, Brie, Comté, Camembert, and Reblochon.\n"
     ]
    }
   ],
   "source": [
    "from phospho.lab.language_models import get_sync_client\n",
    "\n",
    "# Create a client\n",
    "ollama = get_sync_client(\"ollama\")\n",
    "\n",
    "try:\n",
    "  # Let's check we can reach your local Ollama API\n",
    "  response = ollama.chat.completions.create(model='mistral', messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is the best French cheese? Keep your answer short.',\n",
    "    },\n",
    "  ])\n",
    "  print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")\n",
    "  print(\"You need to have a local Ollama server running to continue and the mistral model downloaded. \\nRemove references to Ollama otherwise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the results with the alternative configurations\n",
    "\n",
    "To run the jobs on multiple models at the same time, we will simply set up our job with a different configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "workload = lab.Workload()\n",
    "\n",
    "# Setup the configs for our job\n",
    "class EventConfig(lab.JobConfig):\n",
    "    event_name: str\n",
    "    event_description: str\n",
    "    # Model are ordered from the least desired to the most desired\n",
    "    # The default model is set to be the \"reference\"\n",
    "    model: Literal[\"openai:gpt-4\", \"mistral:mistral-large-latest\", \"mistral:mistral-small-latest\", \"ollama:mistral\"] = \"openai:gpt-4\"\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=EventConfig(\n",
    "            event_name=\"Question Answering\",\n",
    "            event_description=\"User asks a question to the assistant\",\n",
    "            model=\"openai:gpt-4\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the workload in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai:gpt-4'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the default model is currently set to \"openai:gpt-4\"\n",
    "workload.jobs[0].config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's also run the pipeline on alternative models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alternative results with the Mistral API and Ollama\n",
    "await workload.async_run_on_alternative_configurations(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the workload to figure out which model is better. \n",
    "\n",
    "Note that this can actually work with any set of parameters, not just models. It's a flexible way to perform a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: [0.8666666666666667, 0.7333333333333333, 0.6]\n"
     ]
    }
   ],
   "source": [
    "workload.optimize_jobs(accuracy_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what model was picked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral:mistral-large-latest'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the new model_id (if it has changed)\n",
    "workload.jobs[0].config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can run our pipeline with roughly the same accuracy using a smaller model. That's a lot of time, compute and money saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our workload on the full dataset, with optimized parameters\n",
    "\n",
    "Now that we have benchmarked different models for our Event detection pipeline, let's run the optimal configuration on a larger chunk of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset[\"train\"].select(range(200)) # Here you can just leave it as dataset[\"train\"] if you want to use the whole dataset\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
      "event_detection call to OpenAI API failed : Error code: 429 - {'message': 'Requests rate limit exceeded'}\n"
     ]
    }
   ],
   "source": [
    "# The job will be runned with the best model (mistral:mistral-small-latest in our case)\n",
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results\n",
    "\n",
    "Now, we were trying to see which share of the dataset is actually a question. Let's get the results as a dataframe and visualize them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_answering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ed8d794d627041398b2ffa6b2aa22c44</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5d44f42a604c4c93b2b72c466ded9924</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2afa8d650c5b427a8d655495da1da0f0</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f91acb1c813a444b9fd91bc325e2b669</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ba70422146747b7b18c94d01f853dd3</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da6c6502ee014934a78d64dac285bbfb</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02575f7bc8ca4ffa8a1d36e6a7216e84</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05c57c22b9604034a767d4225c6a2c98</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813ae652695b4762aabc2583be210c22</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0c5cf2ea780434eb2877b0a00b317e3</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question_answering\n",
       "ed8d794d627041398b2ffa6b2aa22c44              False\n",
       "5d44f42a604c4c93b2b72c466ded9924               True\n",
       "2afa8d650c5b427a8d655495da1da0f0               True\n",
       "f91acb1c813a444b9fd91bc325e2b669               True\n",
       "6ba70422146747b7b18c94d01f853dd3               True\n",
       "...                                             ...\n",
       "da6c6502ee014934a78d64dac285bbfb               None\n",
       "02575f7bc8ca4ffa8a1d36e6a7216e84               True\n",
       "05c57c22b9604034a767d4225c6a2c98               True\n",
       "813ae652695b4762aabc2583be210c22               True\n",
       "d0c5cf2ea780434eb2877b0a00b317e3               True\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using .results_df() we can get a pandas dataframe with the results\n",
    "workload.results_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many questions in this dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_answering\n",
       "False    88\n",
       "True     75\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='question_answering'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "You can use the `lab` to run other tasks, such as:\n",
    "- Named Entity Recognition\n",
    "- Sentiment Analysis\n",
    "- Evaluations\n",
    "- And more!\n",
    "\n",
    "You can also play around with differnet models, different hyperparameters, and different datasets.\n",
    "\n",
    "You want to have such analysis on your own LLM app, in real time? Check out the cloud hosted version of phospho, available on [phospho.ai](https://phospho.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
